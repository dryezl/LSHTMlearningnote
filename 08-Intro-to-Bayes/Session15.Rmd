> Many statistical models also have anterograde amnesia. As the models move from one cluster - individual, group, location - in the data to another, estimating parameters for each cluster, they forget everything about the previous cluster. ... These models implicitly assume that nothing learned about any one category informs estimates for the other categories -- the parameters are independent of one another and learn from completely separate proportions of the data. This would be like forgetting you had ever been in a cafe, each time you go to a new cafe. Cafes do differ, but they are also alike. 
> ~ Richard McElreath

其實，當我們開始使用回歸模型時，最推薦的就是從多層回歸模型入手，把它當作一種應該實施的默認選項。當然的確非多層回歸的簡單模型在一些場合下就能夠勝任數據分析的過程給出滿意的結果，但事實上更多時候你會發現多層回歸模型會更加出色的幫助我們理解這個世界。所以最好的狀態其實是，我們先從多層回歸模型入手分析數據，隨着分析的深入，過程中我們可能發現不再需要多層模型結構就能完成分析任務。這其實好過我們從一開始就忽略掉了多層回歸模型的這一關鍵的可能性。

## 多層數據實例：蝌蚪和青蛙數據 multilevel tadpoles

```{r introBayes15-01, cache=TRUE}
data("reedfrogs")
d <- reedfrogs
str(d)
```


現在我們只關心上述數據中生存下來的蝌蚪數量 `surv`，和開始時的蝌蚪數量 `density`。該數據包涵了很多的方差 variance。這些方差可能來自不同的實驗條件，或者未知的原因。所以，假設每一行數據中的10只蝌蚪，被放在了不同的水池裏，也就是說，上面的數據中我們有48個水池做重複的實驗。於是該數據就可以被理解爲是重複相似的實驗，但是每次的實驗又有一些微妙的不同。每一個水池，就是一個數據的層級 'cluster'。如果我們忽略這個層級的概念，我們可能就忽略掉了他們本身在實驗開始之時的基線生存狀況 (baseline survival) 本身可能存在的不確定性 (variation)。這個不確定性，或者叫基線生存狀況的方差可能掩蓋住一些重要的發現。如果我們允許每個水池擁有自己單獨的其實狀態，也就是函數的截距，但是假如僅僅使用啞變量的方法 dummy variable，那其實我們就掉進了進行性健忘症的陷阱裏。因爲雖然他們是不同的水池做的實驗，但是一個水池的結果其實是能提示或者告訴我們其他水池的實驗結果的一些信息的，而不是完全地相互獨立毫無關聯性。

所以我們需要的其實是一個同時能夠允許每個水池的蝌蚪生存擁有自己的起始狀態，也就是函數的截距，且同時考慮到他們之間是有關聯性的，也就是這些截距之間是有一定的方差的。這樣的模型就被叫做隨機截距模型 (varing intercepts models)，這樣的模型是最簡單的多層回歸模型。下面的模型用於預測每個不同的水池中實驗過後蝌蚪的生存狀況 (mortality) ：

$$
\begin{aligned}
S_i  & \sim \text{Binomial}(N_i, p_i) \\ 
\text{logit}(p_i) & = \alpha_{\text{TANK}[i]}  & [\text{unique log-odds for each tank}] \\ 
\alpha_j & = \text{Normal}(0, 1.5)  & \text{for } j = 1, \dots, 48
\end{aligned}
$$

這個模型很容易可以編碼成爲 Stan 模型：


```{r introBayes15-02, cache=TRUE}
# make the tank cluster variable
d$tank <- 1:nrow(d)

dat <- list(
  S = d$surv, 
  N = d$density, 
  tank = d$tank
)
```

```{r introBayes15-0201, cache=TRUE, results="hide", eval=FALSE}
# approximate posterior

m13.1 <- ulam(
  alist(
    S ~ dbinom( N, p ), 
    logit(p) <- a[tank], 
    a[tank] ~ dnorm(0, 1.5)
  ), data = dat, chains = 4, log_lik = TRUE
)

saveRDS(m13.1, "../Stanfits/m13_1.rds")
```



```{r introBayes15-03, cache=TRUE}
m13.1 <- readRDS("../Stanfits/m13_1.rds")
precis(m13.1, depth = 2)
```


你會看見模型的運算結果是告訴我們 48 個池塘本身的基線生存狀況，也就是有 48 個截距。但是 `m13.1` 並不是一個多層回歸模型，下面的模型中關鍵部分的加入才使得這個模型變得更加有意義：


$$
\begin{aligned}
S_i & \sim \text{Binomial}(N_i, p_i)  \\ 
\text{logit}(p_i) & = \alpha_{\text{TANK}[i]} \\ 
\alpha_j          & \sim \text{Normal}(\color{blue}{\bar{\alpha}, \sigma}) & [\text{adaptive prior}] \\ 
\color{blue}{\bar{\alpha}} & \color{blue}{\sim \text{Normal}(0, 1.5)}      & [\text{prior for average tank}] \\
\color{blue}{\sigma}       & \color{blue}{\sim \text{Exponential}(1)}      & [\text{prior for standard deviation of tanks}]
\end{aligned}
$$

上述模型中值得注意的是，除了允許不同水池的基線生存狀況，也就是截距可以各不相同，我們還允許這些截距之間存在聯繫。也就是這些截距本身是服從正（常）態分佈的，該正（常）態分佈的均值是 $\bar{\alpha}$，標準差是 $\sigma$。這個截距服從的正（常）態分佈的參數，也有自己的先驗概率分佈。我們把這樣的參數叫做超參數 hyperparameters，他們是參數的參數，他們的先驗概率分佈被叫做超先驗 hyperpriors。我們可以用下面的代碼來運行這個模型：


```{r introBayes15-04, cache=TRUE, results="hide", eval=FALSE}
m13.2 <- ulam(
  alist(S ~ dbinom( N, p ), 
  logit(p) <- a[tank], 
  a[tank] ~ dnorm(a_bar, sigma), 
  a_bar ~ dnorm( 0, 1.5 ), 
  sigma ~ dexp( 1 )
  ), data = dat, chains = 4, log_lik = TRUE
)
saveRDS(m13.2, "../Stanfits/m13_2.rds")
```


先比較一下這兩個模型之間的模型信息差別：

```{r introBayes15-05, cache=TRUE}
m13.2 <- readRDS("../Stanfits/m13_2.rds")
compare( m13.1, m13.2 )
```

從兩個模型之間的比較結果來看，首先，`m13.2` 只有 21 個有效的參數，比起實際的參數個數 50 個少了很多。這是因爲對這些截距增加了超參數的限制之後，他們受到了更多的約束，更加趨近於彼此。我們可以看看這個模型給出的截距分佈的超參數的事後概率分佈估計：


```{r  introBayes15-06, cache=TRUE}
precis(m13.2, depth = 2, pars = c("a_bar", "sigma"))
```


這裡的截距分佈的超參數的事後估計其實給出了十分精確的估計，其均值在 1.34 左右，標準差是1.62，這說明了不同的水池之間的關係十分近似。也就是說，我們使用這個多層回歸模型，讓模型自己從數據中去學習並獲得截距和截距之間的關係。這比起一開始我們自己給 `m13.1` 設定的標準差 `1.5` 還要激進。於是這個多層回歸模型事實上給模型參數的估計增加了更多的限制。


為了加深我們對這個激進的超參數的理解，我們把這兩個模型 `m13.1, m13.2` 給出的估計結果繪製成圖形來觀察：



```{r introBayes15-fig01, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="Empirical proportions of surviviors in each tadpolle tank, shown by the filled blue points, plotted with the 48 per-tank parameters from the multilevel model, shown by the black circles. The dashed line locates the average proportion of survivors across all tanks. The vertical lines divide tanks with different initial densities of tadpoles: small tanks (10 tadpoles), medium tanks (25), and large tanks (35). In every tank, the posterior mean from the multilevel model is closer to the dashed line than the empirical proportion is. This reflects the pooling of information across tanks, to help with inference.", fig.align='center'}
# extract Stan Samples
post <- extract.samples(m13.2)
# post <- extract.samples(m13.1)


# compute mean intercept for each tank
d$propsurv.est <- logistic( apply( post$a, 2, mean ))

#  display raw proportions surviving in each tank
plot( d$propsurv, ylim = c(0, 1), pch = 16, xaxt = 'n', 
      xlab = 'tank', ylab = 'proportion survival', 
      col = rangi2, bty = "n")
axis(1, at = c(1, 16, 32, 48), labels = c(1, 16, 32, 48))


# overlay posterior means
points( d$propsurv.est )

# mark posterior mean probability across tanks 
abline( h = mean(inv_logit(post$a_bar)), lty = 2)

# draw vertical dividers between tank densities
abline( v = 16.5, lwd = 0.5 )
abline( v = 32.5, lwd = 0.5 )
text( 8, 0, 'small tanks')
text( 16 + 8, 0, 'medium tanks')
text( 32 + 8, 0, 'large tanks')
```




圖 \@ref(fig:introBayes15-fig01) 中，橫軸是水池的編號，從左往右依次是從 1 到 48 號水池；縱軸是水池中蝌蚪生存下來的比例。圖中藍色的點是原始數據點，也就是實際觀察值 `propsurv`。黑色的鏤空點則是模型估計的每個水池的截距。水平的橫虛線是估計的所有水池的蝌蚪存活概率的平均值 $\alpha$。而圖中的縱向的實線是把水池按照實驗開始時的蝌蚪密度計算的不同類型的池子，從小，中，到大三種類型的池子，各16個。不難注意到我們能看見多層回歸模型給出的推測值都相對觀察值更靠近總體平均生存概率。看起來似乎是黑色鏤空的圓點都更加靠近數據分佈的中心，平均值附近。這種現象又被叫做縮水現象 shrinkage，這是由於增加了超參數之後的多層回歸模型的參數估計受到的限制性的調整 regularization。其次，我們也發現在圖左側，也就是起始蝌蚪密度較小的水池裏，多層回歸模型估計的生存概率值更加靠近總體平均值，也就是縮水得更加明顯，距離觀察數據比密度大的水池要遠。也就是說，在小的水池裏，我們更加容易發現模型估計值和觀察值之間的差別，但是在其實密度大的水池中，觀察值和模型估計值更加接近。最後，如果藍色的點離虛線的總體均值越遠，它和黑色點，多層回歸模型估計值之間的差別越大。

上述三種現象其實是在告訴我們一件很重要的事，也就是把信息綜合起來的話，每一個層級的參數估計都會受益得到提升和改善 (pooling information across clusters to improve estimates) 。這裏的綜合信息 pooling information 的意義是，每一個水池的數據，每一個層級的數據，都含有能提高和改善其他層級參數估計信息 each tank provides information that can be used to improve the estimates for all of the other tanks。這是因爲我們假設了每個水池的截距 log-odds 雖有變化但不獨立而是服從某個正（常）態分佈。有了這個分佈的假設，貝葉斯估計就能幫助我們共享信息給不同的數據層級。


那麼，模型估計的這些青蛙的總體生存概率的分佈是怎樣的呢？我們可以從它對應的模型事後概率分佈中獲得結果繪製成圖。我們先繪製事後概率分佈中前100個 $\alpha, \sigma$ 組合的平均存活率的分佈：


```{r introBayes15-fig02, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="The inferred population of survival across tanks. 100 Gaussian distributions of the log-odds of survival, sampled from the posteriro of m13.2.", fig.align='center'}
# show first 100 populations in the posterior 
plot( NULL, xlim = c(-3, 4), ylim = c(0, 0.35), 
      xlab = "log-odds survive", 
      ylab = "Density")

for( i in 1:100 ) 
  curve( dnorm(x, post$a_bar[i], post$sigma[i]), add = TRUE, 
         col = col.alpha("black", 0.2))
```


圖 \@ref(fig:introBayes15-fig02) 告訴我們，均值 $\alpha$ 和它對應的標準差 $\sigma$ 都是有相當程度不確定性的 (uncertainty)。


```{r  introBayes15-fig03, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="Survival probabilities for 8000 new simulated tanks, averaging over the posterior distribution in previous figure.", fig.align='center'}
# sample 8000 imaginary tanks from the posterior distribution 
sim_tanks <- rnorm(80000, post$a_bar, post$sigma)

# transform to probability and visualize
dens( inv_logit(sim_tanks), lwd = 2, adj = 0.1, 
      xlab = "probability survive")
```

























