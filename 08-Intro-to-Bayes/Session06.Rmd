這裡的簡單筆記來自經典教學書 [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/)。


## 為什麼正（常）態分佈是正常的 why normal distribution is normal?


> Keep in mind that using a model is not equivalent to swearing an oath on it. The golem is your servant, not the other way around.
> ~ Richard McElreath


概率分佈如果是離散型的，如二項分佈，描述它的分佈函數被叫做概率質量函數 (probability mass functions)。當概率分佈是連續型的，如正（常）態/高斯分佈，描述它的分佈函數被叫做概率密度函數 (probability density functions)。概率密度本身的取值是可以大於1的，如：

```{r}
dnorm(0, 0, 0.1) # mean = 0, sd = 0.1
```

是讓 R 幫助我們計算 $p(0 | 0, (\sqrt{0.1})^2)$ 的取值。其結果差不多要等於4。這並沒有出錯，**概率密度**的概念其實是累積概率的增長速率 (the rate of change in cumulative probability)。所以在概率密度函數的平滑曲線上，累積概率增加的越快的區間，它的增長速率，也就是概率密度很容易超過1。但是，概率密度函數的曲線下面積，則永遠不可能超過1。

## 身高的高斯模型 Gaussian model of height


```{r introBayes06-01, cache=TRUE, message=FALSE}
# library(rethinking)
data("Howell1")

d <- Howell1
str(d)

precis(d)

# filter out non-adults

d2 <- d[ d$age >= 18, ]

```


寫下身高的模型，

$$
\begin{aligned}
h_i & \sim \text{Normal}(\mu, \sigma) &\text{     [Likelihood]} \\
\mu & \sim \text{Normal}(178, 20)     &\text{     [prior for } \mu] \\
\sigma & \sim \text{Uniform}(0, 50)          &\text{     [prior for } \sigma]\\
\end{aligned}
$$

觀察一下我們設定的先驗概率分佈的形狀：

```{r introBayes06-02, cache=TRUE, message=FALSE,fig.asp=.7, fig.width=6,  fig.cap='The shape for prior distribution of the mean.', fig.align='center', out.width='90%'}
curve( dnorm( x, 178, 20), from = 100, to = 250, xlab = ~mu)
```

```{r introBayes06-03, cache=TRUE, message=FALSE,fig.asp=.7, fig.height=3.5, fig.width=4, fig.cap='The shape for prior distribution of the standard deviation.', fig.align='center', out.width='90%'}

curve( dunif( x, 0, 50), from = -10, to = 60, xlab = ~sigma)
```

當你選定了參數 $h, \mu, \sigma$ 的先驗概率分佈時，你其實同時選定了他們的聯合分佈 (joint distribution)。從這個聯合先驗概率分佈中採集一些樣本，可以輔助我們判斷選擇這樣的先驗概率分佈是否是合適/不壞的。

```{r introBayes06-04, cache=TRUE, message=FALSE,fig.asp=.7, fig.height=3, fig.width=4, fig.cap='The shape for samplings from prior distribution of the height generated from the joint distrition chosen for the mean and standard deviation.', fig.align='center', out.width='90%'}
sample_mu <- rnorm(10000, 178, 20)
sample_sigma <- runif(10000, 0, 50)
prior_h <- rnorm(10000, sample_mu, sample_sigma)
dens( prior_h )
```


看起來身高的先驗概率分佈似乎合乎常理，可以暫且認為均值和標準差的先驗概率選擇並不太糟糕。


當你刻意去選擇更加寬泛的先驗概率分佈給均值時，可能導致不合理的數據出現。如下圖 \@ref(fig:introBayes06-05) 中顯示的那樣，當刻意給身高的均值 $\mu$ 賦予更加“不含信息”，或者模糊的先驗概率 $\text{Normal}(178, 100)$，時給出的身高的先驗概率分佈中出現了不少身高為負數的情況，這就是不合理的先驗概率分佈。

```{r introBayes06-05, cache=TRUE, message=FALSE, fig.height=3, fig.width=4, fig.cap='The shape for samplings from prior distribution of the height generated from the joint distrition chosen for the mean and standard deviation, with flatter and less information prior for the mean (mu ~ N(178, 100)).', fig.align='center', out.width='90%', fig.asp=.7}
sample_mu <- rnorm(10000, 178, 100)
prior_h <- rnorm(10000, sample_mu, sample_sigma)
dens( prior_h )
```


模型的貝葉斯定理表達式：


$$
\text{Pr}(\mu, \sigma | h) = \frac{\prod_i \text{Normal}(h_i|\mu, \sigma)\text{Normal}(\mu | 178, 20)\text{Uniform}(\sigma|0,50)}{\int\int \prod_i \text{Normal}(h_i|\mu, \sigma)\text{Normal}(\mu | 178, 20)\text{Uniform}(\sigma|0,50) d\mu d\sigma}
$$


### 小方格估計法近似事後概率分佈


```{r  introBayes06-06, cache=TRUE}
# 在 150 - 160 之間產生100個數據區間
mu_list <- seq( from = 150, to = 160, length.out = 100)

# 在 7-9 之間產生 100 個數據區間
sigma_list <- seq( from  = 7, to = 9, length.out = 100)

# 用上面生成的方差和均值交叉做出 10000 個均值和方差的數據格子
post <- expand.grid( mu = mu_list, sigma = sigma_list)

# 在對數尺度上用加法運算，會節約計算成本
post$LL <- sapply( 1:nrow(post), function(i) sum(
  dnorm( d2$height, post$mu[i], post$sigma[i], log = TRUE)
))

# 計算取對數下的事後概率分佈的分母部分 
post$prod <- post$LL + 
  dnorm( post$mu, 178, 20, log = TRUE) + 
  dunif(post$sigma, 0, 50, log = TRUE)

# Scale all of the log-products by the maximum log-product

post$prob <- exp( post$prod - max(post$prod))

```


可以繪製簡單的事後均值，事後標準差的等高圖分佈，登高線本身使用上面代碼計算的身高的相對事後概率分佈。

```{r introBayes06-07, fig.asp=.7, fig.width=6,  fig.cap='Simple contour plot of the posterior means and standard deviations of height with relative posterior probability distribution. ', fig.align='center', out.width='90%', cache=TRUE}
contour_xyz( post$mu, post$sigma, post$prob)
```

```{r introBayes06-08, fig.asp=.7, fig.width=6,  fig.cap='Simple heat map plot of the posterior means and standard deviations of height with relative posterior probability distribution. ', fig.align='center', out.width='90%', cache=TRUE}
image_xyz(post$mu, post$sigma, post$prob)
```


### 從計算獲得的事後概率分佈中採樣


```{r introBayes06-09, fig.asp=.7, fig.width=6,  fig.cap='Samples from the posterior distribution for the heights data.', fig.align='center', out.width='90%', cache=TRUE}

sample.rows <- sample(1 : nrow(post), size = 10000, replace = TRUE,
                      prob = post$prob)
sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows]

plot(sample.mu, sample.sigma, cex = 0.7, pch = 16, 
     col = col.alpha(rangi2, 0.2))
```

```{r}
dens(sample.mu)
dens(sample.sigma)
PI(sample.mu)
PI(sample.sigma)
```

### 使用二次方程近似法


使用R語言定義我們的身高模型：


$$
\begin{aligned}
h_i & \sim \text{Normal}(\mu, \sigma) &\text{     [Likelihood]} \\
\mu & \sim \text{Normal}(178, 20)     &\text{     [prior for } \mu] \\
\sigma & \sim \text{Uniform}(0, 50)          &\text{     [prior for } \sigma]\\
\end{aligned}
$$


```{r introBayes06-10, cache=TRUE, message=FALSE}
flist <- alist(
  height ~ dnorm( mu, sigma ), 
  mu     ~ dnorm( 178, 20 ), 
  sigma  ~ dunif( 0 , 50 )
)

m4.1 <- quap( flist, data = d2)

precis( m4.1 )
```


繪製該數據中身高曲線之間的關係圖：


```{r introBayes06-11, fig.height=4, fig.asp=.7, fig.width=4.5, fig.cap='Plot adults height against weight.', fig.align='center', out.width='90%', cache=TRUE} 
# plot(d2$height, d2$weight)
ggplot(data = d2, 
       aes(x = weight, y = height)) +
  geom_point(shape = 1, size = 2) +
  theme_bw() +
  theme(panel.grid = element_blank())
```

如何使用身高來做體重的預測變量，從而建立一個簡單線型回歸模型？


$$
\begin{aligned}
h_i & \sim \text{Normal}(\mu_i, \sigma)  & [\text{likelihood}]\\
\mu_i & = \alpha + \beta (x_i - \bar{x}) & [\text{linear model}]\\
\alpha & \sim \text{Normal}(178, 20)    & [\text{prior for } \alpha]\\
\beta & \sim \text{Normal}(0, 10)       & [\text{prior for } \beta]\\   
\sigma & \sim \text{Uniform}(0, 50)     & [\text{prior for } \sigma]
\end{aligned}
$$

### 關於 $\beta$ 的先驗概率 Priors

代碼來自書中(P95 Rcode 4.38)：

```{r introBayes06-12, cache=TRUE, fig.asp=.7, fig.width=6, fig.cap='Prior predictive simulation for the height and weight model. Simulation using the beta ~ N(0, 10) prior.', fig.align='center', out.width='90%'}
set.seed(2971)
N <- 100 #n number of simulation lines

a <- rnorm( N, 178, 20)
b <- rnorm( N, 0, 10)

plot( NULL, xlim = range(d2$weight) , ylim = c(-100, 400), 
      xlab = "weight", ylab = "height")
abline( h = 0, lty = 2)
abline( h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0, 10)" )

xbar <- mean(d2$weight)

for ( i in 1:N) curve( a[i] + b[i]*(x - xbar) ,
  from = min(d2$weight), to = max(d2$weight), add = TRUE, 
  col = col.alpha("black", 0.2)
  )

```


相同圖形使用 tidyverse 和 ggplot2 來製作(參考 [Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition](https://bookdown.org/content/4857/geocentric-models.html#a-gaussian-model-of-height)：

```{r introBayes06-13, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='Prior predictive simulation for the height and weight model. Simulation using the beta ~ N(0, 10) prior.', fig.align='center', out.width='90%'}
set.seed(4)
# how many lines would you like?
n_lines <- 100

lines <-
  tibble(n = 1:n_lines,
         a = rnorm(n_lines, mean = 178, sd = 20),
         b = rnorm(n_lines, mean = 0,   sd = 10)) %>% 
  tidyr::expand(tidyr::nesting(n, a, b), weight = range(d2$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2$weight)))

head(lines)

lines %>% 
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, size = 1/3) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("b ~ dnorm(0, 10)") +
  theme_classic()
```

可見這樣的身高作為先驗概率明顯是不合理的，因為有大量的數據低於 0 cm，甚至於大於人類的極限身高272cm。如何將這個關於身高的先驗概率合理化呢？解決方法的一種是使用對數正（常）態分佈。對數正（常）態分佈，其實就是指，一組取了對數之後的數值服從正（常）態分佈。

$$
\beta \sim \text{Log-Normal}(0, 1)
$$

```{r introBayes06-14, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='The density shape for lognormal (0,1), it is the distribution whose logarithm is normally distributed.', fig.align='center'}
# b <- rlnorm(10000, 0, 1)
# dens(b, xlim = c(0, 5), adj = 0.1)
set.seed(4)
tibble(b = rlnorm(10000, mean = 0, sd = 1)) %>% 
  ggplot(aes(x = b)) + 
  geom_density(fill = "grey70") +
  coord_cartesian(xlim = c(0, 5)) +
  theme_classic()
```

我們還可以把採集的對數正（常）態分佈樣本取對數，和標準正（常）態分佈做一個圖形上的比較。

```{r introBayes06-15, cache=TRUE, fig.height=6, fig.width=4.5, fig.cap='The density shape for log(lognormal (0, 1)) and normal(0, 1), they are the same.', fig.align='center'}
set.seed(4)

tibble(rnorm = rnorm(100000, mean = 0, sd = 1),
       `log(rlognorm)` = log(rlnorm(100000, mean = 0, sd = 1))) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) + 
  geom_density(fill = "grey70") + 
  coord_cartesian(xlim = c(-3, 3)) + 
  theme_classic() + 
  facet_wrap( ~ name, nrow = 2)

```



使用對數正（常）態分佈作為 $\beta$ 的先驗概率分佈，試試看它給出的身高預測範圍大致是怎樣的。


```{r introBayes06-16, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap="Prior predictive simulation for the height and weight model. Simulation using the beta ~ log-normal(0, 1) prior, within much reasonable human ranges.", fig.align='center'}
set.seed(2971)
N <- 100 #n number of simulation lines

a <- rnorm( N, 178, 20)
# b <- rnorm( N, 0, 10)
b <- rlnorm( N, 0, 1)

plot( NULL, xlim = range(d2$weight) , ylim = c(-100, 400), 
      xlab = "weight", ylab = "height")
abline( h = 0, lty = 2)
abline( h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dlognorm(0, 1)" )

xbar <- mean(d2$weight)

for ( i in 1:N) curve( a[i] + b[i]*(x - xbar) ,
  from = min(d2$weight), to = max(d2$weight), add = TRUE, 
  col = col.alpha("black", 0.2)
  )
```

使用 ggplot2 做一邊相同的事。

```{r introBayes06-17, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap="Prior predictive simulation for the height and weight model. Simulation using the beta ~ log-normal(0, 1) prior, within much reasonable human ranges.", fig.align='center'}
# make a tibble to annotate the plot
Text <-
  tibble(weight = c(34, 43),
         height = c(0 - 25, 272 + 25),
         label  = c("Embryo", "World's tallest person (272 cm)"))

# simulate
set.seed(4)

tibble(n = 1:n_lines,
       a = rnorm(n_lines, mean = 178, sd = 20),
       b = rlnorm(n_lines, mean = 0, sd = 1)) %>% 
  tidyr::expand(tidyr::nesting(n, a, b), weight = range(d2$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2$weight))) %>% 
  
  # plot
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, size = 1/3) +
  geom_line(alpha = 1/10) +
  annotate(geom = "text", 
           x = c(34, 43), y = c(-25, 297), 
           label = c("Embryo", "World's tallest person (272 cm)"),
           size = 5) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("log(b) ~ dnorm(0, 1)") +
  theme_classic()
```

> A serious problem in contemporary applied statistics is "p-hacking," the practice of adjusting the model and the data to achieve a desired result.
> The desired result is usually a p-value less than 5%.
>
> `r tufte::quote_footer('--- [Richard McElreath](https://xcelab.net/rm/)')`


於是我們現在把修改了 $\beta$ 的先驗概率分佈之後的模型定義：


$$
\begin{aligned}
h_i & \sim \text{Normal}(\mu_i, \sigma)  & [\text{likelihood}]\\
\mu_i & = \alpha + \beta (x_i - \bar{x}) & [\text{linear model}]\\
\alpha & \sim \text{Normal}(178, 20)    & [\text{prior for } \alpha]\\
\beta & \sim \text{Log-Normal}(0, 1)       & [\text{prior for } \beta]\\   
\sigma & \sim \text{Uniform}(0, 50)     & [\text{prior for } \sigma]
\end{aligned}
$$

於是用這個模型來獲取我們需要的事後概率分佈的方法就顯而易見了：

```{r introBayes06-18, cache=TRUE}
# define the average weight, x-bar
xbar <- mean(d2$weight)

# fit model

m4.3 <- quap(
  alist(
    height ~ dnorm( mu, sigma ),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1), 
    sigma ~ dunif( 0, 50 )
  ), data = d2
)

precis(m4.3)
```


```{r introBayes06-19, cache=TRUE, message=FALSE}

d2 <- d2 %>% 
  mutate(weight_c = weight - mean(weight))

b4.3 <- 
  brm(data = d2, 
      family = gaussian, 
      height ~ 1 + weight_c,
      prior = c(prior(normal(178, 20), class = Intercept), 
                prior(lognormal(0, 1), class = b), 
                prior(uniform(0, 50), class = sigma)), 
      iter = 28000, warmup = 27000, chains = 4, cores = 4,
      seed = 4)

b4.3
```

觀察各個參數事後樣本之間的協方差 (covariances)，幾乎可以忽略不計。

```{r introBayes06-20, cache=TRUE, message=FALSE}
round( vcov(m4.3), 3)

posterior_samples(b4.3) %>%
  dplyr::select(-lp__) %>%
  cov() %>%
  round(digits = 3)
```


```{r introBayes06-21, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='Both marginal posteriors and the covariance.', fig.align='center', out.width='90%'}
pairs(m4.3)
```

```{r introBayes06-22, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='Both marginal posteriors and the covariance.', fig.align='center', out.width='90%'}
pairs(b4.3)
```


繪製事後估計獲得的均值給出的回歸直線。

```{r introBayes06-23, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='Height in centimeters plotted against weight in kilograms, with the line at the posterior mean plotted in black.', fig.align='center', out.width='90%'}

plot( height ~ weight, data = d2, col = rangi2)
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve( a_map + b_map * (x - xbar), add =  TRUE)

```

ggplot2繪圖結果：

```{r introBayes06-24, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='Height in centimeters plotted against weight in kilograms, with the line at the posterior mean plotted in black.', fig.align='center', out.width='90%'}
d2 %>%
  ggplot(aes(x = weight_c, y = height)) +
  geom_abline(intercept = fixef(b4.3)[1], 
              slope     = fixef(b4.3)[2]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  theme_classic()
```



為了顯示斜率和截距本身的不確定性，我們來從參數的事後聯合概率分佈中採集一些樣本出來做觀察：

```{r introBayes06-25, cache=TRUE}
post <- extract.samples( m4.3 )

post[1:5, ]

postb <- posterior_samples(b4.3)
postb %>% 
  slice(1:5)
```

這些 a, b 的無數配對，就是該線型回歸給出的事後回歸線的截距和斜率的無數的組合，每一個組合，是一條事後回歸直線。這些事後截距和事後斜率的均值，是圖 \@ref(fig:introBayes06-24) 和 \@ref(fig:introBayes06-23) 中的那條事後均值提供的直線。事實上，這些截距和斜率本身的不確定性才是更加值得關注的。

下面的計算過程是先使用 `d2` 數據中前10個人的身高體重做相同的事後回歸直線分析，然後逐漸增加數據量，你會觀察到這些事後斜率和事後截距的不確定性在發生的一個動態變化：


```{r introBayes06-26, cache=TRUE}
N <- 10
dN <- d2[1:N, ]
mN <- quap(
  alist(
    height ~ dnorm( mu, sigma ), 
    mu <- a + b*(weight - mean(weight)), 
    a ~ dnorm( 178, 20 ), 
    b ~ dlnorm( 0, 1 ), 
    sigma ~ dunif( 0, 50 )
  ), data = dN
)
```


接下來繪製 `mN` 模型中給出的事後回歸直線的一部分：

```{r introBayes06-27, cache=TRUE, fig.asp=.7, fig.width=6,  fig.cap='Samples from the quadratic approximate posterior distribution for the height/weight model with 20 samples of data. 20 Lines sampled from the posterior distribution.', fig.align='center'}
# extract 20 samples from the posterior 
post <- extract.samples( mN, n = 20)

# display raw data and sample size

plot(dN$weight, dN$height, 
     xlim = range(d2$weight), ylim = range(d2$height), 
     col = rangi2, xlab = "weight", ylab = "height")
mtext(concat( "N = ", N)) 

# plot the lines with transparency 

for ( i in 1:20) 
  curve( post$a[i] + post$b[i] * (x - mean(dN$weight)), 
         col = col.alpha("black", 0.3), add = TRUE)

```

增加樣本量，你只需要修改上述 `N = 10` 中的數字，然後逐個繪圖就能觀察。下面的代碼使用 `brms` 包來實現：


```{r introBayes06-28, cache=TRUE, message=FALSE}
N <- 10 

b4.3 <- 
  brm(data =  d2 %>% 
        slice(1:N), 
      family = gaussian, 
      height ~ 1 + weight_c, 
      prior = c(prior(normal(178, 20), class = Intercept), 
                prior(lognormal(0, 1), class = b), 
                prior(uniform(0, 50), class = sigma)), 
      iter = 28000, warmup = 27000, chains = 4, cores = 4, 
      seed = 4)

N <- 50 
b4.3_50 <- 
  brm(data =  d2 %>% 
        slice(1:N), 
      family = gaussian, 
      height ~ 1 + weight_c, 
      prior = c(prior(normal(178, 20), class = Intercept), 
                prior(lognormal(0, 1), class = b), 
                prior(uniform(0, 50), class = sigma)), 
      iter = 28000, warmup = 27000, chains = 4, cores = 4, 
      seed = 4)

N <- 150 
b4.3_150 <- 
  brm(data =  d2 %>% 
        slice(1:N), 
      family = gaussian, 
      height ~ 1 + weight_c, 
      prior = c(prior(normal(178, 20), class = Intercept), 
                prior(lognormal(0, 1), class = b), 
                prior(uniform(0, 50), class = sigma)), 
      iter = 28000, warmup = 27000, chains = 4, cores = 4, 
      seed = 4)

N <- 352 
b4.3_352 <- 
  brm(data =  d2 %>% 
        slice(1:N), 
      family = gaussian, 
      height ~ 1 + weight_c, 
      prior = c(prior(normal(178, 20), class = Intercept), 
                prior(lognormal(0, 1), class = b), 
                prior(uniform(0, 50), class = sigma)), 
      iter = 28000, warmup = 27000, chains = 4, cores = 4, 
      seed = 4)
```



```{r introBayes06-29, cache=TRUE, message=FALSE, fig.asp=.7, fig.width=10,  fig.cap='Samples from the quadratic approximate posterior distribution for the height/weight model with 20 to 352 (increasing) samples of data. 20 Lines sampled from the posterior distribution.', fig.align='center'}
post010 <- posterior_samples(b4.3)
post050 <- posterior_samples(b4.3_50)
post150 <- posterior_samples(b4.3_150)
post352 <- posterior_samples(b4.3_352)
p1 <- 
  ggplot(data =  d2[1:10 , ], 
         aes(x = weight_c, y = height)) +
  geom_abline(intercept = post010[1:20, 1], 
              slope     = post010[1:20, 2],
              size = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2$weight_c),
                  ylim = range(d2$height)) +
  labs(subtitle = "N = 10")

p2 <-
  ggplot(data =  d2[1:50 , ], 
         aes(x = weight_c, y = height)) +
  geom_abline(intercept = post050[1:20, 1], 
              slope     = post050[1:20, 2],
              size = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2$weight_c),
                  ylim = range(d2$height)) +
  labs(subtitle = "N = 50")

p3 <-
  ggplot(data =  d2[1:150 , ], 
         aes(x = weight_c, y = height)) +
  geom_abline(intercept = post150[1:20, 1], 
              slope     = post150[1:20, 2],
              size = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2$weight_c),
                  ylim = range(d2$height)) +
  labs(subtitle = "N = 150")

p4 <- 
  ggplot(data =  d2[1:352 , ], 
         aes(x = weight_c, y = height)) +
  geom_abline(intercept = post352[1:20, 1], 
              slope     = post352[1:20, 2],
              size = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2$weight_c),
                  ylim = range(d2$height)) +
  labs(subtitle = "N = 352")

(p1 + p2 + p3 + p4) &
  scale_x_continuous("weight",
                     breaks = c(-10, 0, 10),
                     labels = c(1, 2, 3)) &
  theme_classic()
```


可以看見，當數據越多，事後直線估計給出的穩定性越好，不確定性越小。

```{r introBayes06-30, cache=TRUE}
post <- extract.samples( m4.3 )
mu_at50 <- post$a + post$b * (50 - xbar)
head(mu_at50)
```

也就是說，當體重為 50 kg 時，身高的事後概率分佈可以繪製為：

```{r introBayes06-31, cache=TRUE, fig.asp=.7, fig.width=5,  fig.cap='The quadratic approximate posterior distribution of the mean height, when weight is 50 kg. ', fig.align='center'}
dens(mu_at50, col = rangi2, lwd = 2, xlab = "mu | weight = 50")
```

相似的，完成上述圖形的不同代碼可以寫作：

```{r introBayes06-32, cache=TRUE, fig.asp=.7, fig.width=5,  fig.cap='The quadratic approximate posterior distribution of the mean height, when weight is 50 kg. ', fig.align='center'}
mu_at_50 <- postb %>% 
  transmute(mu_at_50 = b_Intercept + b_weight_c + 5.01)

head(mu_at_50)

mu_at_50 %>%
  ggplot(aes(x = mu_at_50, y = 0)) +
  stat_halfeye(point_interval = mode_hdi, .width = .95,
               fill = "royalblue") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["height | weight = 50"])) +
  theme_classic()
```


如果說你要給他增加一個可信區間，那麼當體重為 50 kg時，身高的事後預測89% 區間是：

```{r introBayes06-33, cache=TRUE}
PI(mu_at50, prob = 0.89)
```

但是其實我們更希望使用這事後概率分佈來繪製平均事後截距，平均事後斜率給出的直線本身的不確定性（也就是這條直線本身的89%區間）。函數 `link()` 提供了每一名參加實驗對象的事後身高均值的分佈。


```{r introBayes06-34, cache=TRUE}
mu <- link( m4.3 ) 
str(mu)
```


然後我們使用一段體重的區間來做身高的預測值：

```{r introBayes06-35, cache=TRUE}
# define sequence of weights to compute predictions for 
# these values will be on the horizontal axis

weight.seq <- seq( from = 25, to = 70, by = 1)

```

使用 `link()` 函數來計算這些體重的事後身高分佈：

```{r introBayes06-36, cache=TRUE}
# use link to compute mu
# for each sample from posterior 
# and for each weight in weight.seq

mu <- link(m4.3, data = data.frame(weight = weight.seq))
str(mu)
```


繪製這些體重值上的預測事後身高均值：

```{r introBayes06-37, cache=TRUE, fig.width=5.5, fig.height=5,  fig.cap='The first 100 values in the distribution of mu at each weight value.', fig.align='center'}
# use type = "n" to hide raw data
plot( height ~ weight, d2, type = "n")

# loop over samples and plot each mu value

for ( i in 1:100)
  points( weight.seq, mu[i, ], pch = 16, col = col.alpha(rangi2, 0.1))
```

可以看到每一個體重點上的事後身高預測分佈都是一個類似圖 \@ref(fig:introBayes06-31) 那樣的正（常）態分佈。而且也能注意到，身高的事後分佈的不確定性，他們每一個體重值上的事後身高分佈的方差其實是根據不同的身高而有大小區別之分。

```{r introBayes06-38, cache=TRUE, fig.width=5.5, fig.height=5,  fig.cap='The height data with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points.', fig.align='center'}

# summarize the distribution of mu
mu.mean <- apply( mu, 2, mean )
mu.PI <- apply( mu, 2, PI, prob = 0.89)

# plot raw data
# fading out points to make line and interval more visible 
plot( height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))

# plot the MAP line, aka the mean mu for each weight 
lines( weight.seq, mu.mean )

# plot a shaded region for 89% PI

shade( mu.PI, weight.seq )
```


考慮身高本身的方差進來：

```{r introBayes06-39, cache=TRUE, fig.width=5.5, fig.height=5,  fig.cap='89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of mu. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight.', fig.align='center'}

sim.height <- sim(m4.3, data = list(weight = weight.seq))
str(sim.height)

height.PI <- apply( sim.height,  2, PI, prob = 0.89)

# plot raw data

plot( height ~ weight, d2, col = col.alpha(rangi2, 0.5))

# draw MAP line

lines( weight.seq, mu.mean )

# draw HPDI region for line
# mu.HPDI <- HPDI()
shade( mu.PI, weight.seq )

# draw PI for simulated heights

shade( height.PI, weight.seq)

```


